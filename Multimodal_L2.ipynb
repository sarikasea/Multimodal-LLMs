{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoX+UR6wCYo7Y4nwIzfmUV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarikasea/Multimodal-LLMs/blob/main/Multimodal_L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThVKZwaAJxIW",
        "outputId": "8b5e309d-3b8e-49d6-b6a6-70b61d097c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3zSPl7cCbNj",
        "outputId": "4b4fa9e7-6ddb-4a94-d68d-cb87d7d16b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI initialized for project: gen-lang-client-0744583086 in region: us-central1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import vertexai\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "\n",
        "\n",
        "PROJECT_ID = os.environ.get(\"Gemini API\")\n",
        "if not PROJECT_ID:\n",
        "    # Get the project ID from the gcloud config\n",
        "    PROJECT_ID = \"gen-lang-client-0744583086\"\n",
        "    REGION = \"us-central1\"           # Replace with your desired region\n",
        "\n",
        "    vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "    print(f\"Vertex AI initialized for project: {PROJECT_ID} in region: {REGION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GfPAO4suJnKD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf3b8c10",
        "outputId": "e20530dd-e1c8-4b02-c1ca-0789edfcdd03"
      },
      "source": [
        "%%writefile -a utils.py\n",
        "\n",
        "import base64\n",
        "\n",
        "def print_multimodal_prompt(contents):\n",
        "    \"\"\"\n",
        "    Given a contents list, print the prompt based on the modality.\n",
        "\n",
        "    Args:\n",
        "        contents: a list of multimodal content.\n",
        "    \"\"\"\n",
        "    for content in contents:\n",
        "        if \"text\" in content:\n",
        "            print(f\"Text: {content['text']}\")\n",
        "        elif \"inline_data\" in content:\n",
        "            print(f\"Image: {content['inline_data']['mime_type']}\")\n",
        "            # To display the image, you would need to decode the base64 string\n",
        "            # and use a library like PIL or matplotlib.\n",
        "            # For simplicity, we are just printing the mime type here.\n",
        "        else:\n",
        "            print(f\"Unknown content type: {content}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.generative_models import GenerativeModel"
      ],
      "metadata": {
        "id": "XIG8OMWZFnb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\"gemini-2.5-pro\")"
      ],
      "metadata": {
        "id": "Khn80_xXFrje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "call gemini from the cloud into our notebook. project contains the resources"
      ],
      "metadata": {
        "id": "1S7TwZvDEMEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import gemini"
      ],
      "metadata": {
        "id": "xKP96RkADkEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini(\"What is a multimodal model?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "DG17esjVDkIa",
        "outputId": "b9eeac57-3423-4832-a0a8-9fc6bb9033ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.local/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A multimodal model is a machine learning model that can process and integrate information from **multiple data modalities**, meaning it can understand and learn from different types of input data, such as:\\n\\n*   **Text:**  Written language, like sentences, paragraphs, or documents.\\n*   **Images:** Visual data in the form of pixels, representing scenes, objects, or diagrams.\\n*   **Audio:** Sound data, including speech, music, or environmental sounds.\\n*   **Video:**  A sequence of images with audio, capturing dynamic events and interactions.\\n*   **Sensor Data:** Information collected from sensors, such as temperature, pressure, or acceleration.\\n*   **3D Data:** Point clouds, meshes, or voxels representing three-dimensional objects and environments.\\n*   **Graphs:** Data represented as nodes and edges, showing relationships between entities.\\n\\n**Key characteristics of multimodal models:**\\n\\n*   **Joint Representation:** The model learns to create a unified representation of the data from different modalities. This representation captures the relationships and dependencies between the modalities.\\n*   **Modality Fusion:**  The model combines the information from different modalities.  This fusion can happen at various stages:\\n    *   **Early Fusion:** Concatenating or combining features extracted directly from each modality.\\n    *   **Late Fusion:**  Training individual models for each modality and then combining their predictions.\\n    *   **Intermediate Fusion:** Fusing modalities at intermediate layers of the model architecture.\\n*   **Cross-Modal Interaction:** The model understands how the different modalities influence each other.  For example, the text caption of an image can help the model better understand the image's content, and vice versa.\\n*   **Improved Performance:** By leveraging multiple sources of information, multimodal models can often achieve better performance than models that rely on a single modality.\\n\\n**Why are multimodal models important?**\\n\\n*   **More Realistic Understanding:** The real world is inherently multimodal.  Humans naturally process information from multiple senses simultaneously.  Multimodal models allow AI systems to better mimic this ability and gain a more comprehensive understanding of the world.\\n*   **Robustness:** Multimodal models can be more robust to noise or missing information in one modality.  If the image is blurry, the text caption can still provide context.\\n*   **Novel Applications:** Multimodal models open up possibilities for new applications, such as:\\n    *   **Image Captioning:** Generating textual descriptions of images.\\n    *   **Visual Question Answering (VQA):** Answering questions about an image using both visual and textual information.\\n    *   **Speech Recognition:**  Improving speech recognition accuracy by incorporating visual cues like lip movements.\\n    *   **Sentiment Analysis:**  Understanding the sentiment expressed in a video by analyzing both the audio and visual content.\\n    *   **Robotics:**  Enabling robots to navigate and interact with the world using multiple sensors.\\n\\n**Example Architectures:**\\n\\n*   **CNNs (Convolutional Neural Networks) for image processing and RNNs (Recurrent Neural Networks) or Transformers for text processing, combined using attention mechanisms.** This is a common approach for tasks like image captioning.\\n*   **Transformer-based models** are increasingly popular, as they can be adapted to handle multiple modalities using techniques like cross-attention.  Examples include models like CLIP (Contrastive Language-Image Pre-training).\\n*   **Graph Neural Networks (GNNs)** can be used when the data has a graph structure, allowing for reasoning about relationships between entities in different modalities.\\n\\n**Challenges:**\\n\\n*   **Data Alignment:** Ensuring that the data from different modalities is properly aligned in time and space can be challenging.\\n*   **Modality Imbalance:** Some modalities may be more informative or have different levels of noise than others.  Dealing with this imbalance is important for effective training.\\n*   **Scalability:** Training multimodal models can be computationally expensive, especially when dealing with large datasets.\\n*   **Interpretability:** Understanding how the model is integrating information from different modalities can be difficult.\\n\\nIn summary, multimodal models are a powerful approach to machine learning that allows AI systems to reason about and interact with the world in a more natural and intelligent way by combining information from multiple data sources.  They are crucial for building AI systems that can truly understand and respond to the complexity of the real world.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = \"\"\"\n",
        "In short, what is deeplearning.ai,\n",
        "and what can it offer me as a Machine Learning Engineer?\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DZTAwBHhD4VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_1 = model.generate_content(prompt_1, stream=True)\n"
      ],
      "metadata": {
        "id": "Aby3hvvuDkNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "stream=True processes the response as it being generated.\n",
        "With stream=False, you have to wait until the entire response has been generated before it can be proccessed and printed."
      ],
      "metadata": {
        "id": "ULt3Nx2iLGBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for response in response_1:\n",
        "    print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaiSX5j8DkOY",
        "outputId": "c651ac51-609c-4f4e-8661-b470b79b7d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \"Of course. Here is a short\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "usage_metadata {\n",
            "}\n",
            "model_version: \"gemini-2.5-pro\"\n",
            "create_time {\n",
            "  seconds: 1754195088\n",
            "  nanos: 506803000\n",
            "}\n",
            "response_id: \"kOSOaLP3HoGbmecP2dSGwAM\"\n",
            "\n",
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \" summary and the value proposition for you as a Machine Learning Engineer.\\n\\n### In short, what is deeplearning.ai?\\n\\n**deeplearning.ai is an AI education company founded by Andrew Ng.** It provides world-class, accessible online courses and specializations in machine learning, deep learning, and generative AI,\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "usage_metadata {\n",
            "}\n",
            "model_version: \"gemini-2.5-pro\"\n",
            "create_time {\n",
            "  seconds: 1754195088\n",
            "  nanos: 506803000\n",
            "}\n",
            "response_id: \"kOSOaLP3HoGbmecP2dSGwAM\"\n",
            "\n",
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \" primarily hosted on Coursera.\\n\\n---\\n\\n### What it can offer you as a Machine Learning Engineer:\\n\\nFor an MLE, deeplearning.ai is less about learning the basics and more about strategic upskilling, specialization, and staying current.\\n\\n1.  **Mastering MLOps:** Their **MLOps Special\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "usage_metadata {\n",
            "}\n",
            "model_version: \"gemini-2.5-pro\"\n",
            "create_time {\n",
            "  seconds: 1754195088\n",
            "  nanos: 506803000\n",
            "}\n",
            "response_id: \"kOSOaLP3HoGbmecP2dSGwAM\"\n",
            "\n",
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \"ization** is directly targeted at you. It covers the entire production lifecycle: data engineering, model deployment, monitoring, and management. This is crucial for moving beyond model building into robust, scalable production systems.\\n\\n2.  **Specializing in High-Demand Areas:** You can quickly get up to speed in new domains\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "usage_metadata {\n",
            "}\n",
            "model_version: \"gemini-2.5-pro\"\n",
            "create_time {\n",
            "  seconds: 1754195088\n",
            "  nanos: 506803000\n",
            "}\n",
            "response_id: \"kOSOaLP3HoGbmecP2dSGwAM\"\n",
            "\n",
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \".\\n    *   **Generative AI:** The **\\\"Generative AI with LLMs\\\"** and shorter courses on LangChain are essential for staying relevant in the current market.\\n    *   **Domain Expertise:** Deepen your knowledge in areas like NLP, Computer Vision, or Unsupervised Learning with dedicated specializations.\\n\\n\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "usage_metadata {\n",
            "}\n",
            "model_version: \"gemini-2.5-pro\"\n",
            "create_time {\n",
            "  seconds: 1754195088\n",
            "  nanos: 506803000\n",
            "}\n",
            "response_id: \"kOSOaLP3HoGbmecP2dSGwAM\"\n",
            "\n",
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \"3.  **Strengthening Foundational Theory:** Even for experienced engineers, Andrew Ngâ€™s intuitive explanations in the foundational **Deep Learning Specialization** can fill knowledge gaps and solidify your understanding of *why* certain architectures and techniques work.\\n\\n4.  **Staying on the Cutting Edge:** The platform frequently releases short, practical\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "usage_metadata {\n",
            "}\n",
            "model_version: \"gemini-2.5-pro\"\n",
            "create_time {\n",
            "  seconds: 1754195088\n",
            "  nanos: 506803000\n",
            "}\n",
            "response_id: \"kOSOaLP3HoGbmecP2dSGwAM\"\n",
            "\n",
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \" courses on new tools and techniques (e.g., \\\"LangChain for LLM Application Development\\\"). This allows you to quickly learn and apply state-of-the-art technology without a massive time commitment.\\n\\nIn essence, deeplearning.ai helps you evolve from a model-builder to a full-stack ML\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "usage_metadata {\n",
            "}\n",
            "model_version: \"gemini-2.5-pro\"\n",
            "create_time {\n",
            "  seconds: 1754195088\n",
            "  nanos: 506803000\n",
            "}\n",
            "response_id: \"kOSOaLP3HoGbmecP2dSGwAM\"\n",
            "\n",
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \" professional, keeping your skills sharp and aligned with the industry\\'s latest demands.\"\n",
            "    }\n",
            "  }\n",
            "  finish_reason: STOP\n",
            "}\n",
            "usage_metadata {\n",
            "  prompt_token_count: 25\n",
            "  candidates_token_count: 415\n",
            "  total_token_count: 1938\n",
            "  prompt_tokens_details {\n",
            "    modality: TEXT\n",
            "    token_count: 25\n",
            "  }\n",
            "  candidates_tokens_details {\n",
            "    modality: TEXT\n",
            "    token_count: 415\n",
            "  }\n",
            "  thoughts_token_count: 1498\n",
            "}\n",
            "model_version: \"gemini-2.5-pro\"\n",
            "create_time {\n",
            "  seconds: 1754195088\n",
            "  nanos: 506803000\n",
            "}\n",
            "response_id: \"kOSOaLP3HoGbmecP2dSGwAM\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_1 = model.generate_content(prompt_1,\n",
        "                                     stream=True)"
      ],
      "metadata": {
        "id": "x_xMWObFLNys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for response in response_1:\n",
        "    print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1XCGdHlLN_-",
        "outputId": "8c93c6f6-6c32-43c3-b03a-ed9dff300b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course. Here is a short breakdown.\n",
            "\n",
            "### In short, what is deeplearning.ai?\n",
            "\n",
            "**deeplearning.ai\n",
            "** is a leading AI education company founded by Dr. Andrew Ng. Its mission is to provide world-class AI training to a global audience, offering online courses, specializations, and professional certificates primarily through the Coursera platform.\n",
            "\n",
            "---\n",
            "\n",
            "### What it offers you as a Machine Learning Engineer:\n",
            "\n",
            "As an MLE,\n",
            " deeplearning.ai provides a direct and practical path to upskill in the areas most critical to your role.\n",
            "\n",
            "1.  **Foundational Mastery:** The famous **Deep Learning Specialization** ensures you have a rock-solid understanding of the theory behind neural networks (CNNs, RNNs, etc.), which is crucial\n",
            " for debugging and optimizing models.\n",
            "\n",
            "2.  **Crucial MLOps Skills:** This is their most valuable offering for an MLE. The **Machine Learning Engineering for Production (MLOps) Specialization** is designed specifically for your job. It teaches you how to design, build, and maintain production-level ML systems, covering\n",
            " the entire lifecycle from data pipelines and modeling to deployment and monitoring.\n",
            "\n",
            "3.  **Advanced Specializations:** You can dive deep into specific domains like Natural Language Processing (**NLP Specialization**) or the rapidly evolving field of Generative AI (**Generative AI with Large Language Models**).\n",
            "\n",
            "4.  **Staying Current\n",
            ":** They constantly release short, focused courses on cutting-edge topics (e.g., LangChain, diffusion models) and a weekly newsletter (\"The Batch\") to keep you updated on the latest industry news and research.\n",
            "\n",
            "In essence, deeplearning.ai helps you bridge the gap between knowing ML theory and being a\n",
            " highly effective engineer who can **build and deploy robust, production-ready AI systems.**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodality: Image and Text"
      ],
      "metadata": {
        "id": "r6etUjojLXMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.generative_models import (\n",
        "    GenerativeModel,\n",
        "    Image,\n",
        "    Part,\n",
        ")"
      ],
      "metadata": {
        "id": "nXSvH6_pLOLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model = GenerativeModel(\"gemini-2.0-flash\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX3rJT0lLekV",
        "outputId": "252781f3-6e31-4c2c-fec5-cbca3af8b2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.local/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.load_from_file(\"Andrew_power_tools.png\")\n"
      ],
      "metadata": {
        "id": "LIrpProQLhTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_3 = \"Please describe what is in this image?\"\n"
      ],
      "metadata": {
        "id": "PnDAHiAfMFkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contents_image = [image, prompt_3]\n"
      ],
      "metadata": {
        "id": "HIGBL9SJMHzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import print_multimodal_prompt\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "tTN9JglBMKV_",
        "outputId": "0260d553-2d01-490b-d8d1-b84fa4786502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'print_multimodal_prompt' from 'utils' (/content/utils.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2647195860.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_multimodal_prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------Prompt--------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint_multimodal_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'print_multimodal_prompt' from 'utils' (/content/utils.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_vision(contents_image, model=multimodal_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCvO_wU1MSV_",
        "outputId": "e1bd0138-c38f-49dd-c3c0-47eb80283aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a description of the image:\n",
            "\n",
            "The image shows an Asian man holding a hammer in his left hand and a Black+Decker cordless drill with its battery in his right hand. He is smiling and looking directly at the camera. He is wearing a light blue button-down shirt and a lavalier microphone is clipped to his shirt. The background is a plain grey wall, with a light brown surface partially visible in the lower right corner.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = \"dlai-sc-gemini-bucket/pixel8.mp4\"\n",
        "video_uri = f\"gs://{file_path}\"\n",
        "video_url = f\"https://storage.googleapis.com/{file_path}\""
      ],
      "metadata": {
        "id": "3PPy2Pt_MkPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n"
      ],
      "metadata": {
        "id": "NrYkpC36MkYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Video(video_url, width=450)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "d1YsltEeMkiQ",
        "outputId": "957b6078-fabc-4808-fc66-98bf5daa5a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video src=\"https://storage.googleapis.com/dlai-sc-gemini-bucket/pixel8.mp4\" controls  width=\"450\" >\n",
              "      Your browser does not support the <code>video</code> element.\n",
              "    </video>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"\"\"\n",
        "Answer the following questions using the video only:\n",
        " - What is the main person's profession?\n",
        " - What are the main features of the phone highlighted?\n",
        " - Which city was this recorded in?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7ne1AqzgM1UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video = Part.from_uri(video_uri, mime_type=\"video/mp4\")\n",
        "contents_video = [prompt, video]"
      ],
      "metadata": {
        "id": "AVDGS9u_M3is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses_4 = multimodal_model.generate_content(contents_video, stream=True)"
      ],
      "metadata": {
        "id": "hJRQaRBMM9Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for response in responses_4:\n",
        "    print(response.text, end=\"\")"
      ],
      "metadata": {
        "id": "GEgUcQPfM6Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S0l3qMfZNE4A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}